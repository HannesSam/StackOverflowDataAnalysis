{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "*Document your data collection process and the properties of the data here. Implement, using Python code, to load and preprocess your selected dataset.*\n",
    "\n",
    "To acquire a large number of Stack Overflow questions regarding C# you can query the official Stack Overflow database with SQL queries at https://data.stackexchange.com/stackoverflow/queries. These queries can only return 50 000 results at a time so multiple queries have to be made to get all the questions from within a specific period of time. The time frame chosen in this report was 2019-09-22 to 2020-11-08. This period of time was chosen since this is the period of time that C# 8 was the current release of C# and the assumption here was that the documentation regarding new features could be the most lacking.\n",
    "\n",
    "The query used is as follows: <br>\n",
    "```\n",
    "SELECT * FROM posts WHERE Tags LIKE '%c#%' AND posts.CreationDate < 'Insert start data here' AND posts.CreationDate > 'Insert end date here'\n",
    "ORDER BY posts.CreationDate desc\n",
    "```\n",
    "\n",
    "Since this operation has to be done in multiple queries the date input will vary but in the end, you will end up with a number of .CSV files containing the data. In our case, we ended up with three files containing 117 527 questions. These files were then combined into one which is the Data.csv file that you can see in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from the file and output some values to make sure the data read is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# Ignore warnings for a specific error that we can ignore in this application\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data from the data.csv file\n",
    "stackOverflowData = pd.read_csv(\n",
    "    './Data.csv',\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Rows and Columns:\n(117527, 23)\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the dataset read\n",
    "print('Number of Rows and Columns:')\n",
    "print(stackOverflowData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Column names:\nId\nPostTypeId\nAcceptedAnswerId\nParentId\nCreationDate\nDeletionDate\nScore\nViewCount\nBody\nOwnerUserId\nOwnerDisplayName\nLastEditorUserId\nLastEditorDisplayName\nLastEditDate\nLastActivityDate\nTitle\nTags\nAnswerCount\nCommentCount\nFavoriteCount\nClosedDate\nCommunityOwnedDate\nContentLicense\n"
     ]
    }
   ],
   "source": [
    "# Print the column names of the data\n",
    "listOfColumnNames = stackOverflowData.columns.values.tolist()\n",
    "print('Column names:')\n",
    "for name in listOfColumnNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dates from wich this data is produced\n",
    "minValue = stackOverflowData['CreationDate'].min()\n",
    "maxValue = stackOverflowData['CreationDate'].max()\n",
    "print('Dates from wich the data is produced: ' +\n",
    "      minValue + ' to ' + maxValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to read the data into a Data Frame and prepare the data for the analysis phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Data Frame with only the necessary columns\n",
    "df = stackOverflowData[['AcceptedAnswerId', 'Title',\n",
    "                    'CreationDate', 'Body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not implemented yet but here we will remove all rows witch have an accepted answer\n",
    "#df = df.loc[df['AcceptedAnswerId'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the neccessary data to lowercase \n",
    "df['Body'] = df['Body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words that we want to remove from the dataset \n",
    "# We need a method to remove html tags like <p> that are right next to words without a space in between\n",
    "stopWords = ['code', 'gt', 'i', '<p>i']\n",
    "\n",
    "# Remove all the stopwords from the data\n",
    "df['Body'] = df['Body'].apply(lambda x: ' '.join(\n",
    "    [word for word in x.split() if word not in (stopWords)]))\n",
    "\n",
    "# Print to test the remove stopwords function\n",
    "print('Testa StopWords funktionen: \\n')\n",
    "print(df['Body'].head(4) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiteList = ['use', 'list', 'words']\n",
    "df = df[df['Body'].str.contains(\"|\".join(whiteList))]\n",
    "print(df['Body'].head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "*Document you choice and motivation for selected data mining method(s) here. Choose a data mining method(s) to use in Python code to perform an analysis of your chosen dataset. Describe why you chose the method(s) and what interesting things you have found from the analysis.*\n",
    "\n",
    "*Replace the contents of this cell with your own text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the top appearing words. The nrOfWords variable defines how many words the list should contain.\n",
    "nrOfWords = 10\n",
    "rslt = Counter(' '.join(df['Body']).split()).most_common(nrOfWords)\n",
    "\n",
    "# Print out the list created above.\n",
    "print('\\n')\n",
    "for word in rslt:\n",
    "    print('{} = {}'.format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results\n",
    "\n",
    "*Document an evaluation your analysis results and describe how potentially actionable they are.*\n",
    "\n",
    "*Replace the contents of this cell with your own text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule and description of project plan\n",
    "\n",
    "*Rough schedule for the project beyond the pilot study presented in 3-5. This does not have to be advanced, you can simply provide an estimate based upon reported schedules for similar projects in the literature.*\n",
    "\n",
    "*Replace the contents of this cell with your own text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical aspects that need to be considered\n",
    "\n",
    "*Are there ethical aspects that need to be considered? Are there legal implications (e.g., personal data / GDPR)? Are there implications if the case organization is a business, public authority, or nonprofit entity?*\n",
    "\n",
    "*Replace the contents of this cell with your own text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
